# =============================================================================
# å›½ä¼šè­°äº‹éŒ²æ¤œç´¢ãƒ»åˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ - ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯
# =============================================================================
# 
# æ¦‚è¦:
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å›½ä¼šè­°äº‹éŒ²æ¤œç´¢ãƒ»åˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®šç¾©ã—ã¾ã™ã€‚
# å›½ä¼šè­°äº‹éŒ²APIã¨ã®é€šä¿¡ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€åˆ†ææ©Ÿèƒ½ã€å¯è¦–åŒ–æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
# 
# èª²é¡Œ2: Streamlitã‚’ä½¿ã£ãŸã‚¢ãƒ—ãƒªã®é–‹ç™º + å…¬é–‹
# å­¦ç±ç•ªå·: [å­¦ç±ç•ªå·]
# ä½œæˆè€…: [åå‰]
# ä½œæˆæ—¥: 2024å¹´8æœˆ
# 
# ä¸»è¦æ©Ÿèƒ½:
# - å›½ä¼šè­°äº‹éŒ²APIã¨ã®é€šä¿¡ãƒ»æ¤œç´¢
# - æ¤œç´¢çµæœã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»åˆ†æ
# - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ»å½¢æ…‹ç´ è§£æ
# - çµ±è¨ˆåˆ†æãƒ»å¯è¦–åŒ–
# - æ¤œç´¢å±¥æ­´ã®ç®¡ç†
# - ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# 
# ã‚¯ãƒ©ã‚¹æ§‹æˆ:
# - KokkaiSearchApp: ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹
# 
# ä¾å­˜é–¢ä¿‚:
# - streamlit: UIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
# - requests: HTTPé€šä¿¡
# - pandas: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
# - plotly: ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–
# - janome: æ—¥æœ¬èªå½¢æ…‹ç´ è§£æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# - wordcloud: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# - matplotlib: ã‚°ãƒ©ãƒ•æç”»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# 
# APIä»•æ§˜:
# - ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: https://kokkai.ndl.go.jp/api/speech
# - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: keyword, from, until, speaker, meeting
# - ãƒ¬ã‚¹ãƒãƒ³ã‚¹: JSONå½¢å¼ã®è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿
# 
# é–‹ç™ºç’°å¢ƒ:
# - Anacondaä»®æƒ³ç’°å¢ƒ
# - Python 3.8+
# 
# ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 1.1.0
# =============================================================================

import streamlit as st
import requests
import time
import re
from datetime import date, datetime
import json
import pandas as pd
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
import os

# =============================================================================
# æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆè¨­å®š
# =============================================================================
# 
# èª¬æ˜:
# ä¸€éƒ¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã§ã‚‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯å‹•ä½œã—ã¾ã™ã€‚
# å„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€é©åˆ‡ãªåˆæœŸåŒ–ã‚’è¡Œã„ã¾ã™ã€‚

# Janomeï¼ˆæ—¥æœ¬èªå½¢æ…‹ç´ è§£æï¼‰ã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from janome.tokenizer import Tokenizer
except ImportError:
    pass

# WordCloudï¼ˆãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆï¼‰ã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    import numpy as np
except ImportError:
    pass

# =============================================================================
# å®šæ•°å®šç¾©
# =============================================================================

# APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
API_URL = "https://kokkai.ndl.go.jp/api/speech"

# =============================================================================
# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹
# =============================================================================

class KokkaiSearchApp:
    """
    å›½ä¼šè­°äº‹éŒ²æ¤œç´¢ãƒ»åˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
    
    æ©Ÿèƒ½:
    - å›½ä¼šè­°äº‹éŒ²APIã¨ã®é€šä¿¡
    - æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…
    - ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–
    - æ¤œç´¢å±¥æ­´ã®ç®¡ç†
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
    - ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ
    
    å±æ€§:
    - tokenizer: Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    - stop_words: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆ
    - session_state: Streamlitã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
    
    ãƒ¡ã‚½ãƒƒãƒ‰:
    - __init__: åˆæœŸåŒ–
    - search_speeches: è­°äº‹éŒ²æ¤œç´¢
    - analyze_search_results: æ¤œç´¢çµæœåˆ†æ
    - extract_keywords: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    - create_visualizations: å¯è¦–åŒ–ä½œæˆ
    - å„ãƒšãƒ¼ã‚¸è¡¨ç¤ºãƒ¡ã‚½ãƒƒãƒ‰
    """
    
    def __init__(self):
        """
        ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
        
        æ©Ÿèƒ½:
        - ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        - Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        - ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®è¨­å®š
        - åŸºæœ¬è¨­å®šã®èª­ã¿è¾¼ã¿
        
        ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹:
        - search_history: æ¤œç´¢å±¥æ­´
        - search_results: æ¤œç´¢çµæœ
        - analytics_data: åˆ†æãƒ‡ãƒ¼ã‚¿
        """
        
        # =====================================================================
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        # =====================================================================
        
        # æ¤œç´¢å±¥æ­´ã®åˆæœŸåŒ–
        if 'search_history' not in st.session_state:
            st.session_state.search_history = []
        
        # æ¤œç´¢çµæœã®åˆæœŸåŒ–
        if 'search_results' not in st.session_state:
            st.session_state.search_results = None
        
        # åˆ†æãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
        if 'analytics_data' not in st.session_state:
            st.session_state.analytics_data = {}
        
        # =====================================================================
        # JanomeåˆæœŸåŒ–ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ï¼‰
        # =====================================================================
        
        # Janomeãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        try:
            from janome.tokenizer import Tokenizer
            self.tokenizer = Tokenizer()
            
            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆé™¤å¤–ã™ã‚‹å˜èªï¼‰ã®å®šç¾©
            # æ—¥æœ¬èªã®è­°äº‹éŒ²ã«ç‰¹åŒ–ã—ãŸã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆ
            self.stop_words = {
                # åŸºæœ¬å‹•è©ãƒ»åŠ©å‹•è©
                'ã™ã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 'ã•ã›ã‚‹',
                
                # æŒ‡ç¤ºè©ãƒ»æ¥ç¶šè©
                'ã“ã®', 'ãã®', 'ã‚ã®', 'ã©ã®', 'ã¨ã„ã†', 'ã¨ã„ã£ãŸ', 'ã¨ã—ã¦', 'ã«ã¤ã„ã¦',
                'ã«ãŠã„ã¦', 'ã«å¯¾ã—ã¦', 'ã¨ã„ã†ãµã†ã«', 'ã ã¨', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™',
                
                # åŠ©è©ãƒ»å‰¯è©
                'ã§ã', 'ã‚ˆã†', 'ã‚‚ã®', 'ã“ã¨', 'å ´åˆ', 'ä¸­', 'ãŸã‚', 'ã‹ã‚‰', 'ã¾ã§',
                
                # äººç§°ä»£åè©
                'ç§', 'æˆ‘ã€…', 'çš†ã•ã‚“', 'çš†æ§˜', 'ã‚ãªãŸ', 'ã‚ãªãŸæ–¹',
                
                # æ™‚é–“è¡¨ç¾
                'ä»Š', 'ç¾åœ¨', 'ä»Šå›', 'ä»Šåº¦', 'å…ˆã»ã©', 'å…ˆç¨‹', 'æœ¬æ—¥', 'ä»Šæ—¥', 'æ˜¨æ—¥', 'æ˜æ—¥',
                'æ™‚é–“', 'åˆ†', 'ç§’', 'å¹´', 'æœˆ', 'æ—¥', 'é€±', 'å›', 'åº¦', 'ç•ª', 'å·',
                
                # å¿œç­”è¡¨ç¾
                'ã¯ã„', 'ã„ãˆ', 'ãˆãˆ', 'ã†ã‚“', 'ãã†', 'ã„ã‚„', 'ã¾ã‚', 'ã¡ã‚‡ã£ã¨', 'ã‚„ã¯ã‚Š', 'ã‚„ã£ã±ã‚Š',
                
                # å½¹è·ãƒ»æ•¬ç§°
                'å§”å“¡', 'å¤§è‡£', 'è­°å“¡', 'å…ˆç”Ÿ', 'ç·ç†', 'å‰¯', 'ä¼šé•·', 'ç†äº‹', 'é•·', 'éƒ¨é•·', 'èª²é•·',
                'æ§˜', 'ã•ã‚“', 'æ°', 'å›',
                
                # æ³•å¾‹ãƒ»åˆ¶åº¦é–¢é€£
                'ç¬¬', 'ç« ', 'æ¡', 'é …', 'æ³•', 'æ³•å¾‹', 'åˆ¶åº¦', 'æ”¿ç­–',
                
                # æ€è€ƒãƒ»æ„Ÿæƒ…è¡¨ç¾
                'æ€ã†', 'è€ƒãˆã‚‹', 'æ„Ÿã˜ã‚‹', 'è¦‹ã‚‹', 'èã', 'è¨€ã†', 'è©±ã™', 'è¿°ã¹ã‚‹', 'ç”³ã—ä¸Šã’ã‚‹',
                'çŸ¥ã‚‹', 'åˆ†ã‹ã‚‹', 'ç†è§£ã™ã‚‹', 'èª¬æ˜ã™ã‚‹', 'å ±å‘Šã™ã‚‹',
                
                # ãã®ä»–
                'ãªã©', 'ã¨ã‹', 'ã‚„ã‚‰', 'ã‹ã‚‚', 'ã‹ã‚‚ã—ã‚Œãªã„', 'ã§ã—ã‚‡ã†', 'ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“'
            }
        except ImportError:
            # JanomeãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç©ºã®ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
            self.tokenizer = None
            self.stop_words = set()

    # =============================================================================
    # æ¤œç´¢å±¥æ­´ç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
    # =============================================================================

    def save_search_history_to_csv(self):
        """
        æ¤œç´¢å±¥æ­´ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        æ©Ÿèƒ½:
        - ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®æ¤œç´¢å±¥æ­´ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        - JSONå½¢å¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
        - UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ä¿å­˜
        
        æˆ»ã‚Šå€¤:
        - bool: ä¿å­˜æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
        
        ã‚¨ãƒ©ãƒ¼å‡¦ç†:
        - ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ã®å ´åˆ: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        - ç©ºã®å±¥æ­´ã®å ´åˆ: ä½•ã‚‚ã—ãªã„
        """
        if st.session_state.search_history:
            try:
                # æ¤œç´¢å±¥æ­´ã‚’DataFrameã«å¤‰æ›
                history_data = []
                for item in st.session_state.search_history:
                    history_data.append({
                        'timestamp': item.get('timestamp', ''),
                        'params': json.dumps(item.get('params', {}), ensure_ascii=False),
                        'results_count': item.get('results_count', 0)
                    })
                
                # DataFrameã‚’ä½œæˆã—ã¦CSVã«ä¿å­˜
                df = pd.DataFrame(history_data)
                df.to_csv('search_history.csv', index=False, encoding='utf-8-sig')
                
                st.success("âœ… æ¤œç´¢å±¥æ­´ã‚’æ­£å¸¸ã«ä¿å­˜ã—ã¾ã—ãŸ")
                return True
                
            except Exception as e:
                st.error(f"âŒ æ¤œç´¢å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return False
        else:
            st.info("â„¹ï¸ ä¿å­˜ã™ã‚‹æ¤œç´¢å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

    def add_to_search_history(self, search_params, results_count):
        """
        æ¤œç´¢å±¥æ­´ã«è¿½åŠ ã—ã¦CSVã«ä¿å­˜
        
        æ©Ÿèƒ½:
        - æ–°ã—ã„æ¤œç´¢æ¡ä»¶ã¨çµæœæ•°ã‚’å±¥æ­´ã«è¿½åŠ 
        - é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ´åˆã¯æ›´æ–°ï¼‰
        - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è‡ªå‹•ä»˜ä¸
        - CSVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®è‡ªå‹•ä¿å­˜
        
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        - search_params (dict): æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        - results_count (int): æ¤œç´¢çµæœæ•°
        
        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        1. æ–°ã—ã„å±¥æ­´ã‚¢ã‚¤ãƒ†ãƒ ã®ä½œæˆ
        2. é‡è¤‡ãƒã‚§ãƒƒã‚¯
        3. å±¥æ­´ã¸ã®è¿½åŠ /æ›´æ–°
        4. CSVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜
        """
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
        st.write(f"ğŸ’¾ æ¤œç´¢å±¥æ­´ã«ä¿å­˜: {search_params}")
        
        # æ–°ã—ã„å±¥æ­´ã‚¢ã‚¤ãƒ†ãƒ ã®ä½œæˆ
        search_history_item = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'params': search_params,
            'results_count': results_count
        }
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ´åˆã¯æ›´æ–°ï¼‰
        existing_index = None
        for i, item in enumerate(st.session_state.search_history):
            if item.get('params') == search_params:
                existing_index = i
                break
        
        if existing_index is not None:
            # æ—¢å­˜ã®å±¥æ­´ã‚’æ›´æ–°
            st.session_state.search_history[existing_index] = search_history_item
            st.info("ğŸ”„ æ—¢å­˜ã®æ¤œç´¢å±¥æ­´ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
        else:
            # æ–°ã—ã„å±¥æ­´ã‚’è¿½åŠ 
            st.session_state.search_history.append(search_history_item)
            st.success("âœ… æ–°ã—ã„æ¤œç´¢å±¥æ­´ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        self.save_search_history_to_csv()

    def clear_search_history(self):
        """æ¤œç´¢å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        st.session_state.search_history = []
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if os.path.exists('search_history.csv'):
            try:
                os.remove('search_history.csv')
                st.success("æ¤œç´¢å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                st.error(f"æ¤œç´¢å±¥æ­´ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def highlight_text(self, text, keywords_str):
        """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ"""
        if not keywords_str or not text:
            return text

        keywords = keywords_str.split()
        highlighted_text = text

        for keyword in keywords:
            highlighted_text = re.sub(
                re.escape(keyword), 
                lambda m: f'<span style="background-color: #fff3cd; padding: 2px 4px; border-radius: 3px; font-weight: bold;">{m.group(0)}</span>', 
                highlighted_text, 
                flags=re.IGNORECASE
            )
        return highlighted_text

    def search_speeches(self, params):
        """å›½ä¼šä¼šè­°éŒ²APIæ¤œç´¢"""
        try:
            api_params = {
                "maximumRecords": 30,
                "recordPacking": "json"
            }
            api_params.update(params)
            
            response = requests.get(API_URL, params=api_params, timeout=15.0)
            response.raise_for_status()
            
            if response.text:
                return response.json()
            return None
        except requests.exceptions.RequestException as e:
            st.error(f"APIã¸ã®æ¥ç¶šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None
        except ValueError:
            st.error("APIã‹ã‚‰ã®å¿œç­”ãŒä¸æ­£ãªå½¢å¼ã§ã™ã€‚")
            return None

    def extract_keywords(self, text, min_length=2, top_n=50):
        """janomeã‚’ä½¿ã£ã¦å½¢æ…‹ç´ è§£æã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        if not self.tokenizer or not text:
            return {}
        
        keywords = []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æ
        tokens = self.tokenizer.tokenize(text)
        
        for token in tokens:
            # å“è©æƒ…å ±ã‚’å–å¾—
            features = token.part_of_speech.split(',')
            word = token.surface.strip()
            
            # æ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if (len(word) >= min_length and  # æŒ‡å®šæ–‡å­—æ•°ä»¥ä¸Š
                word not in self.stop_words and  # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
                not word.isdigit() and  # æ•°å­—ã®ã¿é™¤å¤–
                features[0] in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and  # å“è©ãƒ•ã‚£ãƒ«ã‚¿
                features[1] not in ['ä»£åè©', 'æ•°', 'æ¥å°¾'] and  # ç´°åˆ†é¡ãƒ•ã‚£ãƒ«ã‚¿
                not re.match(r'^[ã-ã‚“]+$', word)):  # ã²ã‚‰ãŒãªã®ã¿é™¤å¤–
                
                # å‹•è©ã®å ´åˆã¯åŸå½¢ã«å¤‰æ›
                if features[0] == 'å‹•è©' and len(features) >= 7 and features[6] != '*':
                    keywords.append(features[6])
                else:
                    keywords.append(word)
        
        # å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        keyword_counts = Counter(keywords)
        return dict(keyword_counts.most_common(top_n))

    def analyze_meeting_keywords(self, data):
        """ä¼šè­°åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ"""
        if not data or "speechRecord" not in data:
            return {}
        
        records = data["speechRecord"]
        meeting_analysis = {}
        
        # ä¼šè­°åˆ¥ã«ç™ºè¨€ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        meeting_groups = {}
        for record in records:
            meeting_name = record.get('nameOfMeeting', 'ä¸æ˜')
            if meeting_name not in meeting_groups:
                meeting_groups[meeting_name] = []
            meeting_groups[meeting_name].append(record)
        
        # å„ä¼šè­°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        for meeting_name, meeting_records in meeting_groups.items():
            # å…¨ç™ºè¨€ã‚’çµåˆ
            all_speeches = ' '.join([record.get('speech', '') for record in meeting_records])
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keywords = self.extract_keywords(all_speeches)
            
            # ç™ºè¨€è€…ãƒªã‚¹ãƒˆ
            speakers = list(set([record.get('speaker', 'ä¸æ˜') for record in meeting_records]))
            
            meeting_analysis[meeting_name] = {
                'keywords': keywords,
                'speakers': speakers,
                'total_speeches': len(meeting_records),
                'total_characters': len(all_speeches),
                'speeches': meeting_records
            }
        
        return meeting_analysis

    def create_wordcloud(self, keywords):
        """ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ"""
        if not keywords:
            return None
        
        # WordCloudãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        try:
            from wordcloud import WordCloud
            import matplotlib.pyplot as plt
            import matplotlib.font_manager as fm
            import numpy as np
        except ImportError:
            st.warning("âš ï¸ WordCloudãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")
            return None
        
        try:
            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
            font_path = None
            # ã‚·ã‚¹ãƒ†ãƒ ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã™
            fonts = fm.findSystemFonts()
            for font in fonts:
                if any(jp_font in font.lower() for jp_font in ['noto', 'hiragino', 'yu', 'meiryo', 'msgothic']):
                    font_path = font
                    break
            
            wordcloud = WordCloud(
                width=800, 
                height=400,
                background_color='white',
                font_path=font_path,
                colormap='viridis',
                max_words=100,
                relative_scaling=0.5,
                random_state=42
            ).generate_from_frequencies(keywords)
            
            return wordcloud
        except Exception as e:
            st.warning(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None

    def analyze_search_results(self, data):
        """æ¤œç´¢çµæœã®åˆ†æ"""
        if not data or "speechRecord" not in data:
            return {}
        
        records = data["speechRecord"]
        
        # ç™ºè¨€è€…ã®åˆ†æ
        speakers = [record.get('speaker', 'ä¸æ˜') for record in records]
        speaker_counts = Counter(speakers)
        
        # ä¼šè­°ã®åˆ†æ
        meetings = [record.get('nameOfMeeting', 'ä¸æ˜') for record in records]
        meeting_counts = Counter(meetings)
        
        # æ—¥ä»˜ã®åˆ†æ
        dates = []
        for record in records:
            date_str = record.get('date', '')
            if date_str:
                try:
                    date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    dates.append(date_obj)
                except:
                    pass
        
        # ç™ºè¨€ã®é•·ã•åˆ†æ
        speech_lengths = [len(record.get('speech', '')) for record in records]
        
        return {
            'total_records': len(records),
            'speaker_counts': dict(speaker_counts.most_common(10)),
            'meeting_counts': dict(meeting_counts.most_common(10)),
            'dates': dates,
            'speech_lengths': speech_lengths,
            'avg_speech_length': sum(speech_lengths) / len(speech_lengths) if speech_lengths else 0
        }

    def create_visualizations(self, analytics):
        """ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–"""
        if not analytics:
            return
        
        col1, col2 = st.columns(2)
        
        with col1:
            if analytics.get('speaker_counts'):
                st.subheader("ğŸ“Š ç™ºè¨€è€…åˆ¥ä»¶æ•°")
                speakers_df = pd.DataFrame(
                    list(analytics['speaker_counts'].items()), 
                    columns=['ç™ºè¨€è€…', 'ä»¶æ•°']
                )
                fig = px.bar(speakers_df, x='ä»¶æ•°', y='ç™ºè¨€è€…', orientation='h',
                           color='ä»¶æ•°', color_continuous_scale='Blues')
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if analytics.get('meeting_counts'):
                st.subheader("ğŸ›ï¸ ä¼šè­°åˆ¥ä»¶æ•°")
                meetings_df = pd.DataFrame(
                    list(analytics['meeting_counts'].items()), 
                    columns=['ä¼šè­°å', 'ä»¶æ•°']
                )
                fig = px.pie(meetings_df, values='ä»¶æ•°', names='ä¼šè­°å',
                           color_discrete_sequence=px.colors.qualitative.Set3)
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
        
        # æ™‚ç³»åˆ—åˆ†æ
        if analytics.get('dates'):
            st.subheader("ğŸ“… æ™‚ç³»åˆ—åˆ†æ")
            dates_df = pd.DataFrame({'æ—¥ä»˜': analytics['dates']})
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
            st.write(f"ğŸ“Š æ¤œç´¢çµæœã®æ—¥ä»˜ç¯„å›²: {min(dates_df['æ—¥ä»˜'])} ã€œ {max(dates_df['æ—¥ä»˜'])}")
            st.write(f"ğŸ“Š ç·æ—¥æ•°: {len(dates_df)}æ—¥")
            
            # æ—¥ä»˜ã‚’å¹´æœˆã«å¤‰æ›ï¼ˆã‚ˆã‚Šç¢ºå®Ÿãªæ–¹æ³•ï¼‰
            dates_df['å¹´æœˆ'] = dates_df['æ—¥ä»˜'].dt.strftime('%Y-%m')
            monthly_counts = dates_df.groupby('å¹´æœˆ').size().reset_index(name='ä»¶æ•°')
            
            # ãƒ‡ãƒ¼ã‚¿ãŒè¤‡æ•°æœˆã«ã‚ãŸã‚‹å ´åˆã®ã¿è¡¨ç¤º
            if len(monthly_counts) > 1:
                # å¹´æœˆã§ã‚½ãƒ¼ãƒˆ
                monthly_counts = monthly_counts.sort_values('å¹´æœˆ')
                
                fig = px.line(monthly_counts, x='å¹´æœˆ', y='ä»¶æ•°', 
                             title='æœˆåˆ¥ç™ºè¨€ä»¶æ•°ã®æ¨ç§»',
                             markers=True)
                fig.update_layout(
                    height=400,
                    xaxis_title="å¹´æœˆ",
                    yaxis_title="ç™ºè¨€ä»¶æ•°"
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ğŸ“Š æ™‚ç³»åˆ—åˆ†æã«ã¯è¤‡æ•°æœˆã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚ç¾åœ¨ã®æ¤œç´¢çµæœã¯1ãƒ¶æœˆã®ã¿ã§ã™ã€‚")
                
                # 1ãƒ¶æœˆã®ã¿ã®å ´åˆã§ã‚‚ã€æ—¥åˆ¥ã®æ¨ç§»ã‚’è¡¨ç¤º
                st.subheader("ğŸ“… æ—¥åˆ¥ç™ºè¨€ä»¶æ•°ã®æ¨ç§»")
                daily_counts = dates_df.groupby('æ—¥ä»˜').size().reset_index(name='ä»¶æ•°')
                daily_counts = daily_counts.sort_values('æ—¥ä»˜')
                
                fig = px.line(daily_counts, x='æ—¥ä»˜', y='ä»¶æ•°', 
                             title='æ—¥åˆ¥ç™ºè¨€ä»¶æ•°ã®æ¨ç§»',
                             markers=True)
                fig.update_layout(
                    height=400,
                    xaxis_title="æ—¥ä»˜",
                    yaxis_title="ç™ºè¨€ä»¶æ•°"
                )
                st.plotly_chart(fig, use_container_width=True)

    def export_results(self, data, search_params):
        """æ¤œç´¢çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not data or "speechRecord" not in data:
            return None
        
        records = data["speechRecord"]
        export_data = []
        
        for record in records:
            export_data.append({
                'ç™ºè¨€æ—¥': record.get('date', ''),
                'ç™ºè¨€è€…': record.get('speaker', ''),
                'ä¼šè­°å': record.get('nameOfMeeting', ''),
                'é™¢å': record.get('nameOfHouse', ''),
                'ç™ºè¨€å†…å®¹': record.get('speech', '')[:500] + '...' if len(record.get('speech', '')) > 500 else record.get('speech', ''),
                'URL': record.get('speechURL', '')
            })
        
        df = pd.DataFrame(export_data)
        return df

    def search_page(self):
        st.markdown('<h2 class="sub-header">ğŸ” å›½ä¼šè­°äº‹éŒ²æ¤œç´¢</h2>', unsafe_allow_html=True)
        
        # å¾©å…ƒã•ã‚ŒãŸæ¤œç´¢æ¡ä»¶ã‚’å–å¾—
        restored_params = st.session_state.get('re_search_params', {})
        auto_search = st.session_state.get('auto_search', False)
        
        # å¾©å…ƒã•ã‚ŒãŸæ¡ä»¶ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
        if restored_params:
            if auto_search:
                st.info("ğŸ“‹ æ¤œç´¢å±¥æ­´ã‹ã‚‰å¾©å…ƒã•ã‚ŒãŸæ¡ä»¶ã§è‡ªå‹•æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            else:
                st.info("ğŸ“‹ æ¤œç´¢å±¥æ­´ã‹ã‚‰å¾©å…ƒã•ã‚ŒãŸæ¡ä»¶ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚")
            # å¾©å…ƒã•ã‚ŒãŸæ¡ä»¶ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state['current_search_params'] = restored_params
            # å¾©å…ƒã•ã‚ŒãŸæ¡ä»¶ã‚’ã‚¯ãƒªã‚¢
            del st.session_state['re_search_params']
            # è‡ªå‹•æ¤œç´¢ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢
            if auto_search:
                del st.session_state['auto_search']
        
        # ç¾åœ¨ã®æ¤œç´¢æ¡ä»¶ã‚’å–å¾—ï¼ˆå¾©å…ƒã•ã‚ŒãŸæ¡ä»¶ã¾ãŸã¯ä¿å­˜ã•ã‚ŒãŸæ¡ä»¶ï¼‰
        current_params = st.session_state.get('current_search_params', {})
        
        # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
        with st.form("search_form"):
            # å¾©å…ƒã•ã‚ŒãŸæ¡ä»¶ã¾ãŸã¯ä¿å­˜ã•ã‚ŒãŸæ¡ä»¶ã‚’è¨­å®š
            default_keyword = current_params.get("any", "")
            default_speaker = current_params.get("speaker", "")
            default_meeting = current_params.get("nameOfMeeting", "")
            default_house = current_params.get("nameOfHouse", "æŒ‡å®šã—ãªã„")
            
            keyword = st.text_input("ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆANDæ¤œç´¢ï¼‰", 
                                  value=default_keyword,
                                  placeholder="ä¾‹ï¼šãƒ‡ã‚¸ã‚¿ãƒ«æ”¹é© è¦åˆ¶ç·©å’Œ",
                                  help="è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã£ã¦å…¥åŠ›")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                speaker = st.text_input("ğŸ‘¤ ç™ºè¨€è€…å", 
                                      value=default_speaker,
                                      placeholder="ä¾‹ï¼šå²¸ç”°æ–‡é›„")
            with col2:
                name_of_meeting = st.text_input("ğŸ›ï¸ ä¼šè­°å", 
                                              value=default_meeting,
                                              placeholder="ä¾‹ï¼šäºˆç®—å§”å“¡ä¼š")
            with col3:
                # é™¢åã®é¸æŠè‚¢ã‚’è¨­å®š
                house_options = ("æŒ‡å®šã—ãªã„", "è¡†è­°é™¢", "å‚è­°é™¢", "ä¸¡é™¢")
                house_index = 0
                if default_house in house_options:
                    house_index = house_options.index(default_house)
                name_of_house = st.selectbox("ğŸ¢ é™¢å", house_options, index=house_index)
            
            col4, col5 = st.columns(2)
            with col4:
                # æ—¥ä»˜ã®å¾©å…ƒ
                from_date_str = current_params.get("from", "")
                from_date = None
                if from_date_str:
                    try:
                        from_date = datetime.strptime(from_date_str, '%Y-%m-%d').date()
                    except:
                        from_date = None
                from_date = st.date_input("ğŸ“… æ¤œç´¢æœŸé–“ï¼ˆé–‹å§‹æ—¥ï¼‰", value=from_date)
            with col5:
                until_date_str = current_params.get("until", "")
                until_date = None
                if until_date_str:
                    try:
                        until_date = datetime.strptime(until_date_str, '%Y-%m-%d').date()
                    except:
                        until_date = None
                until_date = st.date_input("ğŸ“… æ¤œç´¢æœŸé–“ï¼ˆçµ‚äº†æ—¥ï¼‰", value=until_date)
            
            search_button = st.form_submit_button("ğŸ” æ¤œç´¢å®Ÿè¡Œ", type="primary")
        
        # è‡ªå‹•æ¤œç´¢ãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆã¯è‡ªå‹•çš„ã«æ¤œç´¢ã‚’å®Ÿè¡Œ
        if auto_search and current_params:
            search_button = True
            # è‡ªå‹•æ¤œç´¢ã®å ´åˆã¯æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            search_params = current_params.copy()
        elif search_button:
            # é€šå¸¸ã®æ¤œç´¢ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆ
            search_params = {}
            
            if keyword:
                search_params["any"] = keyword
            if speaker:
                search_params["speaker"] = speaker
            if name_of_meeting:
                search_params["nameOfMeeting"] = name_of_meeting
            if name_of_house != "æŒ‡å®šã—ãªã„":
                search_params["nameOfHouse"] = name_of_house
            if from_date:
                search_params["from"] = from_date.strftime('%Y-%m-%d')
            if until_date:
                search_params["until"] = until_date.strftime('%Y-%m-%d')
        
        # æ—¢å­˜ã®æ¤œç´¢çµæœãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
        if st.session_state.search_results and not (search_button or (auto_search and current_params)):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.success(f"ğŸ“Š å‰å›ã®æ¤œç´¢çµæœãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚")
            with col2:
                if st.button("ğŸ—‘ï¸ æ¤œç´¢çµæœã‚’ã‚¯ãƒªã‚¢", type="secondary"):
                    st.session_state.search_results = None
                    st.session_state.analytics_data = {}
                    if 'meeting_analysis' in st.session_state:
                        del st.session_state.meeting_analysis
                    st.rerun()
            
            data = st.session_state.search_results
            current_keyword = st.session_state.get('current_search_params', {}).get('any', '')
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
            export_df = self.export_results(data, st.session_state.get('current_search_params', {}))
            if export_df is not None:
                csv = export_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“„ æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"kokkai_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
            
            # æ¤œç´¢çµæœè¡¨ç¤º
            for i, record in enumerate(data["speechRecord"]):
                with st.expander(f"ğŸ“ {record['speaker']}ï¼ˆ{record['nameOfMeeting']}ï¼‰- {record['date']}"):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.markdown(f"**ğŸ“… ç™ºè¨€æ—¥:** {record['date']}")
                        st.markdown(f"**ğŸ‘¤ ç™ºè¨€è€…:** {record['speaker']}")
                        st.markdown(f"**ğŸ›ï¸ ä¼šè­°:** {record['nameOfMeeting']}")
                        st.markdown(f"**ğŸ¢ é™¢:** {record.get('nameOfHouse', 'ä¸æ˜')}")
                    
                    with col2:
                        speech_length = len(record['speech'])
                        st.metric("æ–‡å­—æ•°", f"{speech_length:,}")
                    
                    st.markdown("---")
                    
                    # ç™ºè¨€å†…å®¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
                    speech_text = record['speech']
                    highlighted_speech = self.highlight_text(speech_text, current_keyword)
                    
                    st.markdown(
                        f'<div class="speech-card" style="height: 300px; overflow-y: auto;">{highlighted_speech}</div>',
                        unsafe_allow_html=True
                    )
                    
                    st.markdown(f"ğŸ”— [ç™ºè¨€ã®å…¨æ–‡ã¨å‘¨è¾ºè­°äº‹ã‚’èª­ã‚€]({record['speechURL']})")
        
        if search_button or (auto_search and current_params):
            # è‡ªå‹•æ¤œç´¢ã®å ´åˆã¯æ—¢ã«search_paramsãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹
            if not auto_search:
                # é€šå¸¸ã®æ¤œç´¢ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹ç¯‰
                search_params = {}
                
                if keyword:
                    search_params["any"] = keyword
                if speaker:
                    search_params["speaker"] = speaker
                if name_of_meeting:
                    search_params["nameOfMeeting"] = name_of_meeting
                if name_of_house != "æŒ‡å®šã—ãªã„":
                    search_params["nameOfHouse"] = name_of_house
                if from_date:
                    search_params["from"] = from_date.strftime('%Y-%m-%d')
                if until_date:
                    search_params["until"] = until_date.strftime('%Y-%m-%d')

            # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if search_params:
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                st.write(f"ğŸ” æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {search_params}")
                
                # æ¤œç´¢æ¡ä»¶ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state['current_search_params'] = search_params
                
                with st.spinner("ğŸ” æ¤œç´¢ä¸­ã§ã™..."):
                    time.sleep(1)
                    data = self.search_speeches(search_params)
                    
                    if data and "speechRecord" in data and data["speechRecord"]:
                        st.session_state.search_results = data
                        st.session_state.analytics_data = self.analyze_search_results(data)
                        
                        # janomeåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚’å®Ÿè¡Œ
                        if self.tokenizer:
                            st.session_state.meeting_analysis = self.analyze_meeting_keywords(data)
                        
                        # æ¤œç´¢å±¥æ­´ã«è¿½åŠ 
                        self.add_to_search_history(search_params, data['numberOfRecords'])
                        
                        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                        st.success(f"âœ… æ¤œç´¢çµæœãŒ {data['numberOfRecords']} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ï¼ˆæœ€å¤§30ä»¶è¡¨ç¤ºï¼‰")
                        
                        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                        export_df = self.export_results(data, search_params)
                        if export_df is not None:
                            csv = export_df.to_csv(index=False, encoding='utf-8-sig')
                            st.download_button(
                                label="ğŸ“„ æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=csv,
                                file_name=f"kokkai_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                mime="text/csv"
                            )
                        
                        # æ¤œç´¢çµæœè¡¨ç¤º
                        for i, record in enumerate(data["speechRecord"]):
                            with st.expander(f"ğŸ“ {record['speaker']}ï¼ˆ{record['nameOfMeeting']}ï¼‰- {record['date']}"):
                                col1, col2 = st.columns([3, 1])
                                
                                with col1:
                                    st.markdown(f"**ğŸ“… ç™ºè¨€æ—¥:** {record['date']}")
                                    st.markdown(f"**ğŸ‘¤ ç™ºè¨€è€…:** {record['speaker']}")
                                    st.markdown(f"**ğŸ›ï¸ ä¼šè­°:** {record['nameOfMeeting']}")
                                    st.markdown(f"**ğŸ¢ é™¢:** {record.get('nameOfHouse', 'ä¸æ˜')}")
                                
                                with col2:
                                    speech_length = len(record['speech'])
                                    st.metric("æ–‡å­—æ•°", f"{speech_length:,}")
                                
                                st.markdown("---")
                                
                                # ç™ºè¨€å†…å®¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
                                speech_text = record['speech']
                                highlighted_speech = self.highlight_text(speech_text, keyword)
                                
                                st.markdown(
                                    f'<div class="speech-card" style="height: 300px; overflow-y: auto;">{highlighted_speech}</div>',
                                    unsafe_allow_html=True
                                )
                                
                                st.markdown(f"ğŸ”— [ç™ºè¨€ã®å…¨æ–‡ã¨å‘¨è¾ºè­°äº‹ã‚’èª­ã‚€]({record['speechURL']})")
                    else:
                        st.warning("âš ï¸ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’å¤‰ãˆã¦è©¦ã—ã¦ãã ã•ã„ã€‚")
            else:
                st.error("âŒ æ¤œç´¢æ¡ä»¶ã‚’ä½•ã‹ä¸€ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    def meeting_analysis_page(self):
        st.markdown('<h2 class="sub-header">ğŸ›ï¸ ä¼šè­°åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ</h2>', unsafe_allow_html=True)
        
        if not self.tokenizer:
            st.error("âŒ ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ janome ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚")
            st.code("pip install janome")
            return
        
        if 'meeting_analysis' not in st.session_state or not st.session_state.meeting_analysis:
            st.info("ğŸ” åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return
        
        meeting_analysis = st.session_state.meeting_analysis
        
        # ä¼šè­°é¸æŠï¼ˆã™ã¹ã¦ã®ä¼šè­°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ï¼‰
        meeting_options = ["ã™ã¹ã¦ã®ä¼šè­°"] + list(meeting_analysis.keys())
        selected_meeting = st.selectbox(
            "ğŸ“‹ åˆ†æã™ã‚‹ä¼šè­°ã‚’é¸æŠã—ã¦ãã ã•ã„",
            meeting_options,
            index=0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€Œã™ã¹ã¦ã®ä¼šè­°ã€ã‚’é¸æŠ
            help="æ¤œç´¢çµæœã«å«ã¾ã‚Œã‚‹ä¼šè­°ã‹ã‚‰é¸æŠ"
        )
        
        if selected_meeting:
            if selected_meeting == "ã™ã¹ã¦ã®ä¼šè­°":
                # ã™ã¹ã¦ã®ä¼šè­°ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
                all_keywords = {}
                all_speakers = set()
                total_speeches = 0
                total_characters = 0
                
                for meeting_name, meeting_data in meeting_analysis.items():
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµ±åˆ
                    for keyword, count in meeting_data['keywords'].items():
                        if keyword in all_keywords:
                            all_keywords[keyword] += count
                        else:
                            all_keywords[keyword] = count
                    
                    # ç™ºè¨€è€…ã‚’çµ±åˆ
                    all_speakers.update(meeting_data['speakers'])
                    total_speeches += meeting_data['total_speeches']
                    total_characters += meeting_data['total_characters']
                
                # çµ±åˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                meeting_data = {
                    'keywords': dict(sorted(all_keywords.items(), key=lambda x: x[1], reverse=True)),
                    'speakers': list(all_speakers),
                    'total_speeches': total_speeches,
                    'total_characters': total_characters,
                    'speeches': []  # å…¨ç™ºè¨€ãƒ‡ãƒ¼ã‚¿ã¯é‡è¤‡ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ç©ºã«ã™ã‚‹
                }
            else:
                meeting_data = meeting_analysis[selected_meeting]
            
            # ä¼šè­°ã®åŸºæœ¬æƒ…å ±
            if selected_meeting == "ã™ã¹ã¦ã®ä¼šè­°":
                st.markdown("### ğŸ“Š å…¨ä¼šè­°åŸºæœ¬æƒ…å ±")
            else:
                st.markdown("### ğŸ“Š ä¼šè­°åŸºæœ¬æƒ…å ±")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ç™ºè¨€æ•°", f"{meeting_data['total_speeches']}ä»¶")
            with col2:
                st.metric("ç™ºè¨€è€…æ•°", f"{len(meeting_data['speakers'])}äºº")
            with col3:
                st.metric("ç·æ–‡å­—æ•°", f"{meeting_data['total_characters']:,}å­—")
            with col4:
                avg_chars = meeting_data['total_characters'] // meeting_data['total_speeches']
                st.metric("å¹³å‡æ–‡å­—æ•°", f"{avg_chars:,}å­—/ç™ºè¨€")
            
            # ç™ºè¨€è€…ä¸€è¦§
            if selected_meeting == "ã™ã¹ã¦ã®ä¼šè­°":
                st.markdown("### ğŸ‘¥ å…¨ç™ºè¨€è€…ä¸€è¦§")
            else:
                st.markdown("### ğŸ‘¥ ç™ºè¨€è€…ä¸€è¦§")
            
            speakers_text = "ã€".join(meeting_data['speakers'])
            st.write(speakers_text)
            
            st.markdown("---")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
            if meeting_data['keywords']:
                st.markdown("### ğŸ”¤ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
                
                # ã‚¿ãƒ–ã§è¡¨ç¤ºæ–¹æ³•ã‚’åˆ‡ã‚Šæ›¿ãˆ
                tab1, tab2, tab3 = st.tabs(["ğŸ”¤ ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ“ˆ è©³ç´°åˆ†æ"])
                
                with tab1:
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        keywords_df = pd.DataFrame(
                            list(meeting_data['keywords'].items())[:20], 
                            columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°']
                        )
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‹•çš„ã«è¨­å®š
                        if selected_meeting == "ã™ã¹ã¦ã®ä¼šè­°":
                            title = "ã™ã¹ã¦ã®ä¼šè­° - ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ Top20"
                        else:
                            title = f"{selected_meeting} - ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ Top20"
                        
                        fig = px.bar(
                            keywords_df, 
                            x='å‡ºç¾å›æ•°', 
                            y='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 
                            orientation='h',
                            title=title,
                            color='å‡ºç¾å›æ•°',
                            color_continuous_scale='Viridis'
                        )
                        fig.update_layout(height=600)
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        st.markdown("#### ğŸ“‹ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§")
                        for i, (keyword, count) in enumerate(list(meeting_data['keywords'].items())[:15], 1):
                            st.write(f"{i:2d}. **{keyword}** ({count}å›)")
                
                with tab2:
                    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
                    st.markdown("#### â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                    if self.tokenizer:
                        wordcloud = self.create_wordcloud(meeting_data['keywords'])
                        
                        if wordcloud:
                            fig, ax = plt.subplots(figsize=(12, 6))
                            ax.imshow(wordcloud, interpolation='bilinear')
                            ax.axis('off')
                            st.pyplot(fig)
                            plt.close()
                        else:
                            st.warning("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        st.warning("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ janome ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚")
                        st.code("pip install janome")
                
                with tab3:
                    # è©³ç´°åˆ†æ
                    st.markdown("#### ğŸ“ˆ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è©³ç´°åˆ†æ")
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é•·åˆ¥åˆ†å¸ƒ
                    keyword_lengths = [len(k) for k in meeting_data['keywords'].keys()]
                    length_counts = Counter(keyword_lengths)
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        length_df = pd.DataFrame(
                            list(length_counts.items()), 
                            columns=['æ–‡å­—æ•°', 'å˜èªæ•°']
                        ).sort_values('æ–‡å­—æ•°')
                        
                        fig = px.bar(
                            length_df, 
                            x='æ–‡å­—æ•°', 
                            y='å˜èªæ•°',
                            title="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ–‡å­—æ•°åˆ†å¸ƒ"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        # å‡ºç¾é »åº¦åˆ†å¸ƒ
                        freq_counts = Counter(meeting_data['keywords'].values())
                        freq_df = pd.DataFrame(
                            list(freq_counts.items()), 
                            columns=['å‡ºç¾å›æ•°', 'å˜èªæ•°']
                        ).sort_values('å‡ºç¾å›æ•°')
                        
                        fig = px.bar(
                            freq_df, 
                            x='å‡ºç¾å›æ•°', 
                            y='å˜èªæ•°',
                            title="å‡ºç¾é »åº¦åˆ†å¸ƒ"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # çµ±è¨ˆæƒ…å ±
                    st.markdown("#### ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        total_keywords = len(meeting_data['keywords'])
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", f"{total_keywords}")
                    
                    with col2:
                        total_occurrences = sum(meeting_data['keywords'].values())
                        st.metric("ç·å‡ºç¾å›æ•°", f"{total_occurrences}")
                    
                    with col3:
                        avg_occurrence = total_occurrences / total_keywords if total_keywords > 0 else 0
                        st.metric("å¹³å‡å‡ºç¾å›æ•°", f"{avg_occurrence:.1f}")
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ©Ÿèƒ½
                st.markdown("---")
                st.markdown("### ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
                search_keyword = st.text_input("ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢", placeholder="ä¾‹ï¼šãƒ‡ã‚¸ã‚¿ãƒ«")
                
                if search_keyword:
                    matching_keywords = {
                        k: v for k, v in meeting_data['keywords'].items() 
                        if search_keyword.lower() in k.lower()
                    }
                    
                    if matching_keywords:
                        st.success(f"'{search_keyword}' ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ {len(matching_keywords)} å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                        
                        for keyword, count in sorted(matching_keywords.items(), key=lambda x: x[1], reverse=True):
                            st.write(f"â€¢ **{keyword}**: {count}å›")
                    else:
                        st.warning(f"'{search_keyword}' ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            else:
                st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    def analysis_page(self):
        st.markdown('<h2 class="sub-header">ğŸ“Š æ¤œç´¢çµæœåˆ†æ</h2>', unsafe_allow_html=True)
        
        if st.session_state.analytics_data:
            analytics = st.session_state.analytics_data
            
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.markdown(
                    f'<div class="metric-card"><h3>ğŸ“Š ç·ä»¶æ•°</h3><h2>{analytics["total_records"]}</h2></div>',
                    unsafe_allow_html=True
                )
            
            with col2:
                avg_length = int(analytics.get("avg_speech_length", 0))
                st.markdown(
                    f'<div class="metric-card"><h3>ğŸ“ å¹³å‡æ–‡å­—æ•°</h3><h2>{avg_length:,}</h2></div>',
                    unsafe_allow_html=True
                )
            
            with col3:
                unique_speakers = len(analytics.get("speaker_counts", {}))
                st.markdown(
                    f'<div class="metric-card"><h3>ğŸ‘¥ ç™ºè¨€è€…æ•°</h3><h2>{unique_speakers}</h2></div>',
                    unsafe_allow_html=True
                )
            
            with col4:
                unique_meetings = len(analytics.get("meeting_counts", {}))
                st.markdown(
                    f'<div class="metric-card"><h3>ğŸ›ï¸ ä¼šè­°æ•°</h3><h2>{unique_meetings}</h2></div>',
                    unsafe_allow_html=True
                )
            
            st.markdown("---")
            
            # å¯è¦–åŒ–
            self.create_visualizations(analytics)
            
        else:
            st.info("ğŸ“Š åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

    def history_page(self):
        st.markdown('<h2 class="sub-header">ğŸ“š æ¤œç´¢å±¥æ­´</h2>', unsafe_allow_html=True)
        
        # ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", type="secondary"):
                self.clear_search_history()
                st.rerun()
        
        if st.session_state.search_history:
            st.write(f"ğŸ“Š æ¤œç´¢å±¥æ­´: {len(st.session_state.search_history)}ä»¶")
            
            for i, item in enumerate(st.session_state.search_history):
                with st.expander(f"ğŸ• {item['timestamp']} - {item['results_count']}ä»¶"):
                    # æ¤œç´¢æ¡ä»¶ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã§è¡¨ç¤º
                    params = item.get('params', {})
                    if params:
                        st.markdown("#### ğŸ“‹ æ¤œç´¢æ¡ä»¶")
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            if params.get('any'):
                                st.write(f"**ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:** {params['any']}")
                            if params.get('speaker'):
                                st.write(f"**ğŸ‘¤ ç™ºè¨€è€…:** {params['speaker']}")
                            if params.get('nameOfMeeting'):
                                st.write(f"**ğŸ›ï¸ ä¼šè­°å:** {params['nameOfMeeting']}")
                        
                        with col2:
                            if params.get('nameOfHouse'):
                                st.write(f"**ğŸ¢ é™¢å:** {params['nameOfHouse']}")
                            if params.get('from'):
                                st.write(f"**ğŸ“… é–‹å§‹æ—¥:** {params['from']}")
                            if params.get('until'):
                                st.write(f"**ğŸ“… çµ‚äº†æ—¥:** {params['until']}")
                        
                        # è©³ç´°ãªJSONã‚‚è¡¨ç¤ºï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
                        with st.expander("ğŸ“„ è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰"):
                            st.json(params)
                    else:
                        st.warning("æ¤œç´¢æ¡ä»¶ã®è©³ç´°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    
                    # å†æ¤œç´¢ãƒœã‚¿ãƒ³
                    if st.button(f"ğŸ” ã“ã®æ¡ä»¶ã§å†æ¤œç´¢", key=f"re_search_{i}"):
                        # æ¤œç´¢æ¡ä»¶ã‚’å¾©å…ƒã—ã¦æ¤œç´¢ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                        if params:  # æ¤œç´¢æ¡ä»¶ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
                            st.session_state.re_search_params = item['params']
                            st.session_state.auto_search = True  # è‡ªå‹•æ¤œç´¢ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
                            st.session_state.current_page = "ğŸ” æ¤œç´¢"  # æ¤œç´¢ãƒšãƒ¼ã‚¸ã«ç§»å‹•
                            st.success("æ¤œç´¢æ¡ä»¶ã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚è‡ªå‹•çš„ã«æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                            # æ¤œç´¢ãƒšãƒ¼ã‚¸ã«è‡ªå‹•çš„ã«ç§»å‹•
                            st.rerun()
                        else:
                            st.error("æ¤œç´¢æ¡ä»¶ãŒç©ºã®ãŸã‚ã€å†æ¤œç´¢ã§ãã¾ã›ã‚“ã€‚")
        else:
            st.info("ğŸ“š æ¤œç´¢å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    def help_page(self):
        st.markdown('<h2 class="sub-header">â„¹ï¸ ä½¿ã„æ–¹ã‚¬ã‚¤ãƒ‰</h2>', unsafe_allow_html=True)
        
        st.markdown("""
        ### ğŸ” æ¤œç´¢æ©Ÿèƒ½
        - **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢**: ç™ºè¨€å†…å®¹ã‹ã‚‰è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ AND æ¤œç´¢
        - **ç™ºè¨€è€…æ¤œç´¢**: ç‰¹å®šã®æ”¿æ²»å®¶åã§æ¤œç´¢
        - **ä¼šè­°åæ¤œç´¢**: å§”å“¡ä¼šåãªã©ã§æ¤œç´¢
        - **æœŸé–“æŒ‡å®š**: æ—¥ä»˜ç¯„å›²ã§ã®çµã‚Šè¾¼ã¿
        - **é™¢åˆ¥æ¤œç´¢**: è¡†è­°é™¢ãƒ»å‚è­°é™¢ã§ã®çµã‚Šè¾¼ã¿
        
        ### ğŸ›ï¸ ä¼šè­°åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†ææ©Ÿèƒ½
        - **å½¢æ…‹ç´ è§£æ**: janomeãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã‚‹æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆè§£æ
        - **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º**: åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã‹ã‚‰é‡è¦èªå¥ã‚’æŠ½å‡º
        - **ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰**: è¦–è¦šçš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¡¨ç¤º
        - **è©³ç´°çµ±è¨ˆ**: æ–‡å­—æ•°åˆ†å¸ƒã€å‡ºç¾é »åº¦åˆ†æ
        - **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢**: ç‰¹å®šèªå¥ã®é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        
        ### ğŸ“Š åˆ†ææ©Ÿèƒ½
        - **çµ±è¨ˆã‚µãƒãƒªãƒ¼**: æ¤œç´¢çµæœã®åŸºæœ¬çµ±è¨ˆ
        - **ç™ºè¨€è€…åˆ†æ**: ç™ºè¨€è€…åˆ¥ã®ä»¶æ•°ã‚°ãƒ©ãƒ•
        - **ä¼šè­°åˆ†æ**: ä¼šè­°åˆ¥ã®åˆ†é¡å††ã‚°ãƒ©ãƒ•
        - **æ™‚ç³»åˆ—åˆ†æ**: æœˆåˆ¥ç™ºè¨€ä»¶æ•°ã®æ¨ç§»
        
        ### ğŸ’¾ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
        - **CSVå‡ºåŠ›**: æ¤œç´¢çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        - **æ¤œç´¢å±¥æ­´**: éå»ã®æ¤œç´¢æ¡ä»¶ã‚’ä¿å­˜ãƒ»ç¢ºèª
        
        ### ğŸ¨ ç‰¹å¾´
        - **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¤ãƒ©ã‚¤ãƒˆ**: æ¤œç´¢èªå¥ã‚’çµæœå†…ã§å¼·èª¿è¡¨ç¤º
        - **ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³**: ç¾ã—ã„UI/UX
        - **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ**: æ¤œç´¢ã¨åŒæ™‚ã«ãƒ‡ãƒ¼ã‚¿åˆ†æ
        
        ### ğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
        ```bash
        pip install streamlit requests pandas plotly janome wordcloud matplotlib numpy
        ```
        """)
        
        st.markdown("---")
        st.info("""
        **ã€ã”åˆ©ç”¨ä¸Šã®æ³¨æ„ã€‘**
        - ã“ã®ã‚¢ãƒ—ãƒªã¯ã€å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ã®[å›½ä¼šä¼šè­°éŒ²æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ æ¤œç´¢ç”¨API](https://kokkai.ndl.go.jp/api.html)ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
        - ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã€é€£ç¶šã§ã®æ¤œç´¢ã¯ãŠæ§ãˆãã ã•ã„ã€‚
        - å›½ä¼šè­°äº‹éŒ²ã«ãŠã‘ã‚‹å„ç™ºè¨€ã®è‘—ä½œæ¨©ã¯ã€åŸå‰‡ã¨ã—ã¦ãã‚Œãã‚Œã®ç™ºè¨€è€…ã«å¸°å±ã—ã¾ã™ã€‚
        - å½¢æ…‹ç´ è§£ææ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ janome ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚
        - ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ wordcloud ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚
        """)
